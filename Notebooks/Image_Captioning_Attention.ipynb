{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, time, os, warnings\n",
    "import re\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from numpy import array\n",
    "from pickle import load\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text_original):\n",
    "   text_no_punctuation = text_original.translate(string.punctuation)\n",
    "   return(text_no_punctuation)\n",
    "\n",
    "def remove_single_character(text):\n",
    "   text_len_more_than1 = \"\"\n",
    "   for word in text.split():\n",
    "       if len(word) > 1:\n",
    "           text_len_more_than1 += \" \" + word\n",
    "   return(text_len_more_than1)\n",
    "\n",
    "def remove_numeric(text):\n",
    "   text_no_numeric = \"\"\n",
    "   for word in text.split():\n",
    "       isalpha = word.isalpha()\n",
    "       if isalpha:\n",
    "           text_no_numeric += \" \" + word\n",
    "   return(text_no_numeric)\n",
    "\n",
    "def text_clean(text_original):\n",
    "   text = remove_punctuation(text_original)\n",
    "   text = remove_single_character(text)\n",
    "   text = remove_numeric(text)\n",
    "   return(text)\n",
    "\n",
    "def data_limiter(num,total_captions,all_img_name_vector):\n",
    "    train_captions, img_name_vector = shuffle(all_captions,all_img_name_vector,random_state=1)\n",
    "    train_captions = train_captions[:num]\n",
    "    img_name_vector = img_name_vector[:num]\n",
    "    return train_captions,img_name_vector\n",
    "\n",
    "def load_image(image_path):\n",
    "   img = tf.io.read_file(image_path)\n",
    "   img = tf.image.decode_jpeg(img, channels=3)\n",
    "   img = tf.image.resize(img, (224, 224))\n",
    "   img = preprocess_input(img)\n",
    "   return img, image_path\n",
    "\n",
    "def calc_max_length(tensor):\n",
    "   return max(len(t) for t in tensor)\n",
    "\n",
    "def calc_min_length(tensor):\n",
    "   return min(len(t) for t in tensor)\n",
    "\n",
    "def map_func(img_name, cap):\n",
    " img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    " return img_tensor, cap\n",
    "\n",
    "def rnn_type(units):\n",
    "   if tf.test.is_gpu_available():\n",
    "       return tf.compat.v1.keras.layers.CuDNNLSTM(units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "   else:\n",
    "       return tf.keras.layers.GRU(units,\n",
    "                                  return_sequences=True,\n",
    "                                  return_state=True,\n",
    "                                  recurrent_activation='sigmoid',\n",
    "                                  recurrent_initializer='glorot_uniform')\n",
    "\n",
    "def evaluate(image):\n",
    "   attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "   hidden = decoder.reset_state(batch_size=1)\n",
    "   temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "   img_tensor_val = image_features_extract_model(temp_input)\n",
    "   img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "   features = encoder(img_tensor_val)\n",
    "   dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "   result = []\n",
    "\n",
    "   for i in range(max_length):\n",
    "       predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "       attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "       predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "       result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "       if tokenizer.index_word[predicted_id] == '<end>':\n",
    "           return result, attention_plot\n",
    "\n",
    "       dec_input = tf.expand_dims([predicted_id], 0)\n",
    "   attention_plot = attention_plot[:len(result), :]\n",
    "\n",
    "   return result, attention_plot\n",
    "\n",
    "def plot_attention(image, result, attention_plot):\n",
    "   temp_image = np.array(Image.open(image))\n",
    "   fig = plt.figure(figsize=(10, 10))\n",
    "   len_result = len(result)\n",
    "   for l in range(len_result):\n",
    "       temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "       ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "       ax.set_title(result[l])\n",
    "       img = ax.imshow(temp_image)\n",
    "       ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"../input/flickr8k/Images\"\n",
    "dir_Flickr_text = \"../input/flickr8k/captions.txt\"\n",
    "jpgs = os.listdir(image_path)\n",
    "\n",
    "print(\"Total Images in Dataset = {}\".format(len(jpgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(dir_Flickr_text,'r')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "#print(text)\n",
    "datatxt = []\n",
    "for line in text.split('\\n'):\n",
    "    col = line.split(',')\n",
    "    if len(col) == 1:\n",
    "        continue\n",
    "    w = col[0].split(\"#\")\n",
    "    datatxt.append(w + [col[1].lower()])\n",
    "\n",
    "#print(datatxt)\n",
    "data = pd.DataFrame(datatxt,columns=[\"filename\",\"caption\"])\n",
    "data = data[data.filename != 'image']\n",
    "uni_filenames = np.unique(data.filename.values)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npic = 5\n",
    "npix = 224\n",
    "target_size = (npix,npix,3)\n",
    "count = 1\n",
    "\n",
    "fig = plt.figure(figsize=(10,20))\n",
    "for jpgfnm in uni_filenames[10:14]:\n",
    "    filename = image_path + '/' + jpgfnm\n",
    "    captions = list(data[\"caption\"].loc[data[\"filename\"]==jpgfnm].values)\n",
    "    image_load = load_img(filename, target_size=target_size)\n",
    "    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n",
    "    ax.imshow(image_load)\n",
    "    count += 1\n",
    "\n",
    "    ax = fig.add_subplot(npic,2,count)\n",
    "    plt.axis('off')\n",
    "    ax.plot()\n",
    "    ax.set_xlim(0,1)\n",
    "    ax.set_ylim(0,len(captions))\n",
    "    for i, caption in enumerate(captions):\n",
    "        ax.text(0,i,caption,fontsize=20)\n",
    "    count += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "for txt in data.caption.values:\n",
    "    vocabulary.extend(txt.split())\n",
    "print('Vocabulary Size: %d' % len(set(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, caption in enumerate(data.caption.values):\n",
    "   newcaption = text_clean(caption)\n",
    "   data[\"caption\"].iloc[i] = newcaption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_vocabulary = []\n",
    "for txt in data.caption.values:\n",
    "   clean_vocabulary.extend(txt.split())\n",
    "print('Clean Vocabulary Size: %d' % len(set(clean_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./Images/\"\n",
    "all_captions = []\n",
    "for caption  in data[\"caption\"].astype(str):\n",
    "   caption = '<start> ' + caption+ ' <end>'\n",
    "   all_captions.append(caption)\n",
    "\n",
    "all_captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_name_vector = []\n",
    "for annot in data[\"filename\"]:\n",
    "   full_image_path = PATH + annot\n",
    "   all_img_name_vector.append(full_image_path)\n",
    "\n",
    "all_img_name_vector[:10]\n",
    "\n",
    "print(f\"len(all_img_name_vector) : {len(all_img_name_vector)}\")\n",
    "print(f\"len(all_captions) : {len(all_captions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions,img_name_vector = data_limiter(40000,all_captions,all_img_name_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "image_features_extract_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_train = sorted(set(img_name_vector))\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for img, path in tqdm(image_dataset):\n",
    " batch_features = image_features_extract_model(img)\n",
    " batch_features = tf.reshape(batch_features,\n",
    "                             (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    " for bf, p in zip(batch_features, path):\n",
    "   path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "   np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                 oov_token=\"<unk>\",\n",
    "                                                 filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = calc_max_length(train_seqs)\n",
    "min_length = calc_min_length(train_seqs)\n",
    "\n",
    "print('Max Length of any caption : Min Length of any caption = '+ str(max_length) +\" : \"+str(min_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,cap_vector, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "features_shape = 512\n",
    "attention_features_shape = 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "        map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_Encoder(tf.keras.Model):\n",
    "   # This encoder passes the features through a Fully connected layer\n",
    "   def __init__(self, embedding_dim):\n",
    "       super(VGG16_Encoder, self).__init__()\n",
    "       self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "       self.dropout = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)\n",
    "\n",
    "   def call(self, x):\n",
    "       x = self.fc(x)\n",
    "       x = tf.nn.relu(x)\n",
    "       return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rnn_Local_Decoder(tf.keras.Model):\n",
    " def __init__(self, embedding_dim, units, vocab_size):\n",
    "   super(Rnn_Local_Decoder, self).__init__()\n",
    "   self.units = units\n",
    "   self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "   self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                  return_sequences=True,\n",
    "                                  return_state=True,\n",
    "                                  recurrent_initializer='glorot_uniform')\n",
    "   self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "   self.dropout = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)\n",
    "   self.batchnormalization = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "   self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "   self.Uattn = tf.keras.layers.Dense(units)\n",
    "   self.Wattn = tf.keras.layers.Dense(units)\n",
    "   self.Vattn = tf.keras.layers.Dense(1)\n",
    "\n",
    " def call(self, x, features, hidden):\n",
    "   hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "   score = self.Vattn(tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis)))\n",
    "   attention_weights = tf.nn.softmax(score, axis=1)\n",
    "   context_vector = attention_weights * features\n",
    "   context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "   x = self.embedding(x)\n",
    "   x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "   output, state = self.gru(x)\n",
    "   x = self.fc1(output)\n",
    "   x = tf.reshape(x, (-1, x.shape[2]))\n",
    "   x= self.dropout(x)\n",
    "   x= self.batchnormalization(x)\n",
    "   x = self.fc2(x)\n",
    "   return x, state, attention_weights\n",
    "\n",
    " def reset_state(self, batch_size):\n",
    "   return tf.zeros((batch_size, self.units))\n",
    "\n",
    "encoder = VGG16_Encoder(embedding_dim)\n",
    "decoder = Rnn_Local_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "   from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    " mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    " loss_ = loss_object(real, pred)\n",
    " mask = tf.cast(mask, dtype=loss_.dtype)\n",
    " loss_ *= mask\n",
    "\n",
    " return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []\n",
    "\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    " loss = 0\n",
    " hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    " dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    " with tf.GradientTape() as tape:\n",
    "     features = encoder(img_tensor)\n",
    "     for i in range(1, target.shape[1]):\n",
    "         predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "         loss += loss_function(target[:, i], predictions)\n",
    "         dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    " total_loss = (loss / int(target.shape[1]))\n",
    " trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    " gradients = tape.gradient(loss, trainable_variables)\n",
    " optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    " return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "for epoch in range(0, EPOCHS):\n",
    "   start = time.time()\n",
    "   total_loss = 0\n",
    "\n",
    "   for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "       batch_loss, t_loss = train_step(img_tensor, target)\n",
    "       total_loss += t_loss\n",
    "\n",
    "       if batch % 100 == 0:\n",
    "           print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "   loss_plot.append(total_loss / num_steps)\n",
    "   print ('Epoch {} Loss {:.6f}'.format(epoch + 1,total_loss/num_steps))\n",
    "   print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = '../input/flickr8k/Images/2319175397_3e586cfaf8.jpg'\n",
    "\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "# remove <start> and <end> from the real_caption\n",
    "first = real_caption.split(' ', 1)[1]\n",
    "real_caption = 'Two white dogs are playing in the snow'\n",
    "\n",
    "#remove \"<unk>\" in result\n",
    "for i in result:\n",
    "   if i==\"<unk>\":\n",
    "       result.remove(i)\n",
    "\n",
    "for i in real_caption:\n",
    "   if i==\"<unk>\":\n",
    "       real_caption.remove(i)\n",
    "\n",
    "#remove <end> from result        \n",
    "result_join = ' '.join(result)\n",
    "result_final = result_join.rsplit(' ', 1)[0]\n",
    "\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = result\n",
    "\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(f\"BELU score: {score*100}\")\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', result_final)\n",
    "plot_attention(image, result, attention_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "start = time.time()\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "first = real_caption.split(' ', 1)[1]\n",
    "real_caption = first.rsplit(' ', 1)[0]\n",
    "\n",
    "#remove \"<unk>\" in result\n",
    "for i in result:\n",
    "   if i==\"<unk>\":\n",
    "       result.remove(i)\n",
    "\n",
    "#remove <end> from result        \n",
    "result_join = ' '.join(result)\n",
    "result_final = result_join.rsplit(' ', 1)[0]\n",
    "\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = result_final\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', result_final)\n",
    "\n",
    "plot_attention(image, result, attention_plot)\n",
    "print(f\"time took to Predict: {round(time.time()-start)} sec\")\n",
    "\n",
    "Image.open(img_name_val[rid])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
